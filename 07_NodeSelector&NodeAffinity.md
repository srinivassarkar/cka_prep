---

# ğŸš€ KUBERNETES SCHEDULING MASTER LAB

(NodeSelector â†’ Required Affinity â†’ Preferred â†’ Operators â†’ OR Logic â†’ Debugging)

---

# ğŸ”¥ PHASE 0 â€” Reset Everything

```bash
kubectl delete deploy --all
kubectl delete pod --all
```

Check nodes:

```bash
kubectl get nodes -o wide
kubectl get nodes --show-labels
```

ğŸ§  Study:

* Default labels exist (`kubernetes.io/*`)
* Never rely on dynamic labels in production design.

---

# ğŸ”¥ PHASE 1 â€” nodeSelector (Strict & Simple)

## 1ï¸âƒ£ Label Nodes

```bash
kubectl label nodes worker1 storage=ssd
kubectl label nodes worker2 storage=hdd
```

Verify:

```bash
kubectl get nodes --show-labels | grep storage
```

---

## 2ï¸âƒ£ Create nodeSelector Deployment

```bash
vi ns.yaml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ns-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ns
  template:
    metadata:
      labels:
        app: ns
    spec:
      nodeSelector:
        storage: ssd
      containers:
      - name: nginx
        image: nginx
```

Apply:

```bash
kubectl apply -f ns.yaml
kubectl get pods -o wide
```

âœ” All pods on worker1

---

## 3ï¸âƒ£ AND Condition

Add:

```bash
kubectl label nodes worker1 env=prod
```

Modify:

```yaml
nodeSelector:
  storage: ssd
  env: prod
```

âœ” Must match BOTH.

---

## 4ï¸âƒ£ Break It

Remove label:

```bash
kubectl label nodes worker1 env-
```

Pods â†’ Pending.

Check:

```bash
kubectl describe pod <podname>
```

Youâ€™ll see:

```
0/2 nodes available
```

---

## ğŸ§  nodeSelector Core Facts

* Hard rule
* Exact match only
* AND logic only
* No OR
* No weights
* Not retroactive
* Scheduler only checks during scheduling

---

# ğŸ”¥ PHASE 2 â€” Required Node Affinity

Delete:

```bash
kubectl delete deploy ns-deploy
```

Relabel:

```bash
kubectl label nodes worker1 storage=ssd
```

---

## 5ï¸âƒ£ Basic Required Affinity

```bash
vi na.yaml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: na-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: na
  template:
    metadata:
      labels:
        app: na
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: storage
                operator: In
                values:
                - ssd
      containers:
      - name: nginx
        image: nginx
```

âœ” Same behavior as nodeSelector
But more expressive.

---

## ğŸ§  Required Affinity Deep Rules

Inside one `nodeSelectorTerms`:
â†’ matchExpressions = AND

Multiple `nodeSelectorTerms`:
â†’ OR

Not retroactive (IgnoredDuringExecution).

---

# ğŸ”¥ PHASE 3 â€” True OR Logic (Real Upgrade)

Label second node:

```bash
kubectl label nodes worker2 storage=hdd
```

Modify values:

```yaml
values:
- ssd
- hdd
```

Pods now distribute across both nodes.

---

## ğŸ”¥ REAL OR (Multiple Terms)

```yaml
nodeSelectorTerms:
- matchExpressions:
  - key: zone
    operator: In
    values:
    - east
- matchExpressions:
  - key: env
    operator: In
    values:
    - dev
```

Meaning:

```
(zone=east) OR (env=dev)
```

This is commonly misunderstood in interviews.

---

# ğŸ”¥ PHASE 4 â€” Gt / Lt Operator (Integer Comparison)

Clean:

```bash
kubectl delete deploy --all
```

Add numeric labels:

```bash
kubectl label nodes worker1 cpu=8
kubectl label nodes worker2 cpu=2
```

---

## Gt Example

```yaml
- key: cpu
  operator: Gt
  values:
  - "4"
```

âœ” Only worker1

---

## Lt Example

```yaml
- key: cpu
  operator: Lt
  values:
  - "4"
```

âœ” Only worker2

---

## ğŸ§  Gt/Lt Rules

* Works only in affinity
* Label must be numeric string
* If non-numeric â†’ rule fails â†’ Pending

Exam trap area.

---

# ğŸ”¥ PHASE 5 â€” NotIn / DoesNotExist (Anti-Placement)

Label:

```bash
kubectl label nodes worker1 env=prod
kubectl label nodes worker2 env=dev
```

Block dev:

```yaml
- key: env
  operator: NotIn
  values:
  - dev
```

âœ” Avoids worker2

---

DoesNotExist:

```yaml
- key: dedicated
  operator: DoesNotExist
```

âœ” Avoid nodes having that key.

---

## ğŸ§  Operator Memory Table

| Operator     | Meaning                |
| ------------ | ---------------------- |
| In           | Must match values      |
| NotIn        | Must NOT match values  |
| Exists       | Key must exist         |
| DoesNotExist | Key must NOT exist     |
| Gt           | Greater than (numeric) |
| Lt           | Less than (numeric)    |

---

# ğŸ”¥ PHASE 6 â€” Preferred Affinity (Soft + Weight)

Delete old:

```bash
kubectl delete deploy --all
```

Create:

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: storage
          operator: In
          values:
          - ssd
          - hdd
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 10
      preference:
        matchExpressions:
        - key: storage
          operator: In
          values:
          - ssd
    - weight: 5
      preference:
        matchExpressions:
        - key: storage
          operator: In
          values:
          - hdd
```

âœ” More pods on SSD.

---

## ğŸ§  Scheduler Internal Mechanics

Scheduler Phases:

1ï¸âƒ£ Filter phase
â†’ Remove nodes failing required rules

2ï¸âƒ£ Score phase
â†’ Add weight for each preferred match

3ï¸âƒ£ Sum weights

4ï¸âƒ£ Highest score wins

âš  But scoring also includes:

* CPU
* Memory
* Taints
* Other plugins

Affinity is one plugin in scoring stack.

---

# ğŸ”¥ PHASE 7 â€” Debugging Like a Senior

If pod stuck:

```bash
kubectl describe pod <pod>
```

Look for:

```
0/2 nodes available
```

More detailed:

```bash
kubectl get events --sort-by=.metadata.creationTimestamp
```

Youâ€™ll see exact reason:

* Node didn't match affinity
* Insufficient CPU
* Node had taint

This is real production debugging skill.

---

# ğŸš¨ CRITICAL DIFFERENCE TABLE

| Feature     | nodeSelector | requiredAffinity | preferredAffinity |
| ----------- | ------------ | ---------------- | ----------------- |
| Hard rule   | âœ…            | âœ…                | âŒ                 |
| OR logic    | âŒ            | âœ…                | âœ…                 |
| AND logic   | âœ…            | âœ…                | âœ…                 |
| Weight      | âŒ            | âŒ                | âœ…                 |
| Gt/Lt       | âŒ            | âœ…                | âœ…                 |
| Retroactive | âŒ            | âŒ                | âŒ                 |

Only **NoExecute taint** causes eviction.

---

# ğŸ§  PRODUCTION DESIGN PATTERNS

Dedicated hardware
â†’ Taint + Required Affinity

Soft preference
â†’ Preferred only

Environment isolation
â†’ Required affinity + namespace strategy

Avoid dev nodes
â†’ NotIn

Numeric scaling (large nodes only)
â†’ Gt operator

---

# ğŸ§  MASTER MENTAL MODEL

Taints â†’ repel
nodeSelector â†’ strict match
requiredAffinity â†’ strict + logic
preferredAffinity â†’ scoring preference

Scheduler = Filter â†’ Score â†’ Bind

Retroactive?
Only NoExecute taint.

---

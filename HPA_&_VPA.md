# PART 1 â€” HORIZONTAL POD AUTOSCALER (HPA)

---

# ðŸ§  First Principles

HPA changes:

```
replicas count
```

It does NOT change:

* CPU limits
* Memory limits
* Node size

It scales **number of pods**.

---

# ðŸ”¥ LAB 1 â€” Setup HPA Environment

### Step 1 â€” Ensure metrics-server works

```bash
kubectl top nodes
```

If fails â†’ fix metrics-server.

HPA depends on:

```
metrics.k8s.io
```

---

### Step 2 â€” Deploy nginx-hpa.yaml

```bash
kubectl apply -f nginx-hpa.yaml
```

Check:

```bash
kubectl get deploy
```

Should show:

```
1 replica
```

---

# ðŸ”¥ LAB 2 â€” Create HPA

Create:

```bash
kubectl autoscale deployment nginx-deploy \
  --cpu-percent=50 \
  --min=1 \
  --max=5
```

OR YAML way:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deploy
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

Apply.

Check:

```bash
kubectl get hpa
```

---

# ðŸ§  What 50% Means

It means:

```
Current CPU usage / requested CPU
```

Important:

HPA uses:

```
CPU usage Ã· CPU request
```

NOT CPU limit.

---

# ðŸ”¥ VERY IMPORTANT

If requests.cpu not defined:

HPA will NOT work.

It calculates percentage based on requests.

---

# ðŸ”¥ LAB 3 â€” Generate Load

Exec into pod:

```bash
kubectl exec -it <pod-name> -- sh
```

Install curl or use busybox pod to generate load:

```bash
while true; do wget -q -O- http://nginx-svc; done
```

Watch:

```bash
kubectl get hpa -w
```

Youâ€™ll see:

```
Replicas increase
```

Then:

```bash
kubectl get pods
```

Scaling out happens.

---

# ðŸ§  Internal Working of HPA

Every 15 seconds:

1. HPA controller checks metrics API
2. Calculates average CPU utilization
3. Computes desired replicas:

Formula (important):

```
desiredReplicas = currentReplicas Ã— (currentMetric / targetMetric)
```

Example:

* currentReplicas = 2
* current utilization = 100%
* target = 50%

So:

```
2 Ã— (100/50) = 4 replicas
```

---

# ðŸ”¥ Interview Gold

If asked:

> Why did HPA not scale?

You check:

* Is metrics-server running?
* Are CPU requests defined?
* Is load sustained long enough?
* Is maxReplicas reached?

---

# ðŸ”¥ HPA Limitations

* Works best for stateless apps
* Does not scale based on memory reliably
* CPU scaling reactive, not predictive
* Scaling down has cooldown delay

---

# ðŸ”¥ Production Insight

In real clusters:

* HPA + Cluster Autoscaler
* If pods scale but nodes full â†’ pending pods
* Cluster Autoscaler adds nodes

---

# ðŸ”¥ PART 2 â€” CLUSTER AUTOSCALER (Conceptual)

HPA scales pods.
Cluster Autoscaler scales nodes.

Flow:

1. HPA increases replicas
2. Some pods become Pending
3. Cluster Autoscaler detects unschedulable pods
4. Adds new node
5. Pods get scheduled

Thatâ€™s real elasticity.

---

# ðŸ”¥ PART 3 â€” VERTICAL POD AUTOSCALER (VPA)

Now your YAML.

---

# ðŸ§  First Principles

VPA changes:

```
CPU & memory requests
```

NOT replicas.

It can restart pods.

---

# ðŸ”¥ LAB 4 â€” Deploy VPA Setup

Apply:

```bash
kubectl apply -f nginx-vpa.yaml
kubectl apply -f vpa.yaml
```

Check:

```bash
kubectl get vpa
```

Wait some time.

Then:

```bash
kubectl describe vpa nginx-vpa
```

Youâ€™ll see:

```
Recommendation:
  Target:
  Lower Bound:
  Upper Bound:
```

---

# ðŸ§  What Those Fields Mean

Lower Bound â†’ safe minimum
Target â†’ ideal request
Upper Bound â†’ safe max

Uncapped Target â†’ raw calculation

---

# ðŸ”¥ updateMode Explained

```
Off     â†’ recommend only
Initial â†’ set only during pod creation
Auto    â†’ evict + recreate pod with new resources
```

Auto = disruptive (pod restart)

---

# ðŸ”¥ IMPORTANT: VPA + HPA Conflict

They conflict if both control CPU.

Why?

HPA uses CPU request to calculate %.
VPA changes CPU request.

This causes unstable scaling.

Production rule:

* Use HPA for CPU scaling
* Use VPA for memory tuning
* Or use VPA in "Off" mode for recommendations

---

# ðŸ”¥ LAB 5 â€” Observe Evictions

```bash
kubectl get events
```

Youâ€™ll see:

```
VPA evicting pod
```

Pod gets recreated.

---

# ðŸ§  Why Restart Required?

Because:

Resource requests are immutable.
Pod spec change = recreate.

---

# ðŸ”¥ VPA Components (Deep Understanding)

VPA consists of:

1. Recommender
2. Updater
3. Admission Controller

Recommender â†’ calculates
Updater â†’ evicts pods
Admission â†’ injects new requests

---

# ðŸ”¥ HPA vs VPA Quick Brain Table

| Feature              | HPA            | VPA                               |
| -------------------- | -------------- | --------------------------------- |
| Scales               | Replicas       | CPU/Memory                        |
| Restarts pod?        | No             | Yes                               |
| Uses metrics-server? | Yes            | No (uses custom metrics pipeline) |
| Good for             | Stateless apps | Right-sizing                      |

---

# ðŸ”¥ Scaling Strategy in Production

Real-world pattern:

Frontend:

* HPA
* CPU-based scaling

Backend:

* HPA

Databases:

* Vertical scaling (manual)
* Or managed service

Cluster:

* Cluster Autoscaler

---

# ðŸ’£ Interview Traps

1. HPA not working â†’ missing CPU requests
2. HPA scaling not happening â†’ metrics-server missing
3. VPA causing restarts â†’ updateMode Auto
4. VPA + HPA CPU conflict
5. Node not scaling â†’ Cluster Autoscaler missing

---

# ðŸ”¥ Elite-Level Understanding

Elastic system flow:

Load â†‘
â†’ CPU usage â†‘
â†’ HPA scales pods
â†’ Nodes full
â†’ Cluster Autoscaler scales nodes
â†’ Load distributed

Load â†“
â†’ HPA scales down
â†’ Idle nodes removed

Thatâ€™s cloud-native elasticity.

---
